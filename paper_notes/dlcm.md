# [DLCM: Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space](https://arxiv.org/abs/2512.24617)

_January 2026_

tl;dr: Collapse multiple language token into one "concept", reasons in concept space, and then decode back into language tokens.

#### Overall impression
This is better than latent CoT methods such as [Coconut](coconut.md) in that it can be decoded back into langauge space where latent CoT cannot explicitly be decoded into language tokens.

Concpet models essentially are doing specdec not only in last output space but also in model space. 

Caution: The speed up in concept model will canabalize specdec. 

#### Key ideas
- <!--Summaries of the key ideas in nested bullet point. The main areas to focus on model architecture, data, eval.-->

#### Technical details
- <!--Summary of technical details, such as important training details, or bugs of previous benchmarks.-->

#### Notes
- <!--Questions and notes on how to improve the current work-->

