# ICCV 2025


* Towards Comprehensive Reasoning in Vision-Language Models
https://wangywust.github.io/iccv-tutorial-reasoning-vlm/


* Native Multimodal Models: Architecture, Post-Training, and Evaluation 

![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 08.38.56.png)

- Remove vision priors

- Arch
	- [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
	- Continuous tokens
	- Native resoilution from Tile
	- Early fusion
- Training recipe
	- PT, mid-training and SFT

	
- Post training
	- [Visual Jigsaw Post-Training Improves MLLMs](https://arxiv.org/abs/2509.25190)
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 08.52.32.png)
	- dense reconstruction is useful, but too expensive, use jigsaw to do reasoning
	- Image jigsaw: Patch indices in the correct raster scan order
	- Video jigsaw: clip indices in the correct chorno order

- Evaluation


# Driving Simulation from Real-World Data: How Well Can We Render and Drive?
https://realadsim.github.io/2025/
19 Oct 2025 from 09:00 - 12:15 HST. [ 305 A ]
09:10 - 09:40	Keynote-1 [Yue Wang](https://yuewang.xyz/)

Robotics data is expensive
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 09.09.23.png)
	- 1 to 2 orders magnitude more expensive in time and money

- Three ways
	- Use neural simulation
	- Use human data
	- Turn internet data into robotics manipulation data
- How to do neural simulation is the focus of the talk
- Controllable recon of dynamic scenes
- [OmniRe: Omni Urban Scene Reconstruction](https://arxiv.org/abs/2408.16760)
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 09.14.17.png)
	- Gausian splatting
	- Challenges: 
		- freq occlusion and partial observation
		- SMPL parameterization of humans
	- Cyclist/wheelchair Deformed objects: no model so controllability is not as good
	- DriveStudio is the first 3DGS codebase that supports all driving datasets
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 09.22.05.png)
	- Drawbacks: lengthy training for a single scene, no data prior, requires dynamic observations in time

- [STORM: Spatio-Temporal Reconstruction Model for Large-Scale Outdoor Scenes](https://arxiv.org/abs/2501.00602)
	- Feedforward recon

	
- Robot learning from non-robotics data
	- [RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation](https://arxiv.org/abs/2407.04689)
	- Affordance representation
	- Even can turn Tom and Jerry into a drawer demonstration
	- [Robot Learning from Any Images](https://www.arxiv.org/abs/2509.22970)
		- Place robot in the scene
- Learn form Human videos 
	- [UH-1: Learning from Massive Human Videos for Universal Humanoid Pose Control](https://arxiv.org/abs/2412.14172)

Q&A:
- Do we need non-AV data for AV?
	- in-domain data is abundant
	- VLM is leveraging ood data already

- Peter Kontschieder @meta
	- HMD: head mounted device



- Learning to See: Advancing Spatial Understanding for Embodied Intelligence
https://opendrivelab.com/iccv2025/workshop/
Room 312, October 19


- Learning Structured World Models From and For Physical Interactions
	- [Yunzhu Li](https://yunzhuli.github.io/)
	- WM (genie3) <--> Nvidia Warp/Newton
	- Right now genie 3 is too gameish
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 10.54.45.png)
	- EAI community are not trusting their evaluation
	- 从rgbd视频中重建出deformable物体和环境的digital twin，然后在sim里interact with the twin。感觉比遥操作更scalable
	- Manipulation is all about changing the world, and simulating the messy environmment is challenging
	- The "correlation" of the simulation benchmark matters. The order should correlate in simulation and in real world, for best model selection.

	
	
- [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622), Yukang Chen
	- Interative long video generation
	- user inputs time-windowed propmots
	- Environment keeps consistent
	- Frame sink
	- KV-recache

	
- Driving Simulation from Real-World Data: How Well Can We Render and Drive?
https://realadsim.github.io/2025/
Teaching cars to think, by EMMA author, Runsheng Xu
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-19\ at\ 12.01.43.png)
	- [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209) <kbd>IROS 2025</kbd>



- World model: eovling from data closed loop to training closed loop
	- Kun Zhan @ LiAuto
	- 1.5B km of driving data
	- Data closed loop cannot solve all problems alone
	- 3DGS --> Feedforwaerd 3DGS
	- Reconstruction + generation
	- RL Engine, interactive agents are the key challenge
	- Li Auto facts
		- Uses 1MP as input. They want to upgrade to higher resolution but not able to due to compute constraints. 
		- Li Auto is doing FP8 on Thor and is exploring FP4 on Thor for VLA inference. Speculative decoding is also used.
		- Li Auto is doing one-step GRPO-style RL. They pointed out that the timing/keyframe selection for GRPO is critical.
		- Li Auto has ~100 people working on RL gym + RL algo. 90 people are RL gym infra, and 10 are RL algo. 
		- Li Auto's CLE has 10k+ cases, and runs super efficiently with TaT <1 hour. It is also 3DGS based. Same tech as ours, but effiiency is much better. They put a lot of efforts into this. @Parixit Aghera 
		- Eval metrics super accurate without human verification or very little human verification. They are achieving this by human labeling of each case. They tried generic eval (like ours) but found it too noisy to be productive.
		- The RL algo is currently following [RAD](https://arxiv.org/abs/2502.13144) and [CarPlanner](https://arxiv.org/abs/2502.19908).

	
- Pulkit Agrawal @MIT
	- [DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation](https://arxiv.org/abs/2509.04441)

- [Bagel: Emerging Properties in Unified Multimodal Pretraining](https://arxiv.org/abs/2505.14683)
	- Haoqi Fan, https://bagel-ai.org/
	- BAGEL 1, reasoning capability emerges with m;ultimodality training
	- BAGEL world: interleaved reasoning
	- BAGEL light: 130x speed up
	- BAGEL 2
		- ![](/Users/patrliu/Desktop/bagel2_arch.jpg)
		- ![](/Users/patrliu/Desktop/bagel2_arch2.jpg)
		- Use video composer to bridge the gap between Language and Vision
		- [Transfer between Modalities with MetaQueries](https://arxiv.org/abs/2504.06256), by Saining Xie



- [Distillation of Foundation Models for Autonomous Driving](https://wdfm-ad.github.io/iccv25/)
- Jose Alvarez's talk
	- Departing from human demo
	- [Generalized Trajectory Scoring for End-to-end Multimodal Planning](https://arxiv.org/abs/2506.06664)
	- Data problem: Collect or generate?
	- Use MLLM to do data clustering
	- [SSE: Multimodal Semantic Data Selection and Enrichment for Industrial-scale Data Assimilation](https://arxiv.org/abs/2409.13860) <kbd>KDD 2025</kbd>
	- Scaling AV with reasnininbg in the loop
	- Data driven approach with a twist
		- Redefinition of the data coverage
		- Counterfacturals for data ingestion at scale

		
- Machine Learning for validation, scenario encoding, organization and generation
	- Kai Wang@Zoox
	- "The toaster"
	- Validation is the hardest thing we need to solve
	- Trained a scene encoder with reconstruction loss, takes in roads, signals and tracks. 
	- Find patterns
		- [Foundation Models for Rapid Autonomy Validation](https://arxiv.org/abs/2411.03328)
		- Large and small clusters. Have enough granularity to find patterns "nudge around lead car"
	- Patch holes
		- [Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion](https://arxiv.org/abs/2311.02738) <kbd>NeurIPS 2023<kbd>
	- Take aways
		- DL planner and common sense guardrails
		- Human like and smart driving, learning from demo and self-play (RL)
		- Rapid geofence scaling with efficient data infra to enable data driven solution
	- Efficiency and throughput is the core of RL
		- [Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning](https://arxiv.org/abs/2312.15122)
	- Sim2real gap
		- Any discrepancies between sim and real will get exploited by RL
	- Learn to visualize the relevancy of all agents
		- [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)

- 	Data flywheel at Uber
	-  Min Cai@Uber, distinguished eng
	-  Michelangelo, Uber's ML platform
	-  [Waymo's new Scaling law: New Insights for Scaling Laws in Autonomous Driving](https://waymo.com/blog/2025/06/scaling-laws-in-autonomous-driving), 06/2025
	-  Multimodal datalake 
	-  from tabular rows to spatiotempal scenes, one schema to power all
	-  ![](/Users/patrliu/Desktop/uber_data_learning.jpg)
	-  ![](/Users/patrliu/Desktop/uber_multimodal_datalake.jpg)


- Scaling for Motion Forecasting
	- Kratarth Goel@Waymo
	- Long tail cases
		- Extreme weather
		- Uniquque interactions
		- Foreigh objects
		- Unusal behavior
	- for generalization want a scaled up model, but there are limits to how much oine can scale on vehicle
		- Build the most genealizable 
		- Powerful offboard teacher for the onboard driver
	- 3 ingradients of model scale
		- Scale model family: MotionLM (ICCV 2023)
		- Right mix of data: scaling law
		- Distillation
	- Algo
		- <insert algo slide here>
	- From More structure to less structure, and bitter lesson
	- Scaling law
		- Bsaed on MotionLM
		- [Scaling Laws of Motion Forecasting and Planning -- Technical Report](https://arxiv.org/abs/2506.08228)
		- 10x more compute --> 4x more parameters, and 3x more data
		- Irresucible loss term attributeable to uncertainty
	- Does scaling law tranlate to close loop?
		- Yes at least for MotionLM
		- No closed loop training
	- Trajectory distillation
		- [Scaling Motion Forecasting Models with Ensemble Distillation](https://arxiv.org/abs/2404.03843) <kbd>ICRA 2024</kbd>

		

- Building foundaitonal models for tobotics at Tesla 
	- Ashok Elluswamy
	- Arch
		- runs at 36 hz
		- has audio
	- reason for e2e
		- codify human values is difficult
		- interaface between modules is ill defined
		- homogeneous compute with determinstic latency
		- Overall, on the correct side of scaling wrt the bitter lesson
	- Cofiying human values
		- Going over the small puddle
		- interface is hard: intension understanding is hard, very hard to write explicit code
	- Main challenges
		- Curse of dimensionality: 2 billion tokens input to 2 tokens
		- Large data gives extreme generalization --> Brute force
	- Foundation models also predicts manay interpretable outptus
		- 3DGS, feedforward generates 3d gaussian splats, 30 min --> 220 ms. --> This could be a Sanja topic. This is the new occupancy network
		- Natural language. --> Uses template
	- Evaluation: the hardest problem of the three
		- close loop simulator\
		- reevaluate old issues
		- Create adversarial new issues
		- Driving game in the extra low-latency nurec

- Jamie Shotton@wayve
	- 2% of data from London to migrate to Japan
	
	
- [DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation](https://arxiv.org/pdf/2411.04999) [VQBeT author]




## [Foundation Models Meet Embodied Agents](https://foundation-models-meet-embodied-agents.github.io/)

- Foundation Models meet Virtual Agents, by Manling Li 李曼玲, at Northwestern U
- Assessible skills
- Use LLM as frozen, but builds external structures
	- Voyager
- Finetune LLM
- Foundation Models meet Physical Agents: High-Level and Low-level Decision Making	
	- Jiayuan Mao, at UPenn
- Robotic Foundation Models by Yunzhu Li@Columbia
	- https://foundation-models-meet-embodied-agents.github.io/uploads/FM-EA-robotic-foundation-models.pdf
	- Not all data from Open-X dataset are equal. People are selecting their own subset of data.
	- RT1 --> RT2 --> RT2-X --> OpenVLA (first open-source) --> PI-0 (2024/10)
	- Helix (Figure), Hi-Robot (PI), Gemini (Google), Pi-0.5 (PI), GR00T (Nvidia), DYNA-1, LBM (TRI)
	- PI-0.5, and then [Gemini-Robotics-1.5](https://deepmind.google/discover/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/)
- Challenges
	- Challenges: Evaluation
		- Real-world eval is costly and noisy. 2 days to get evaluation. "We have large enough budget such taht we can still make progress."
		- Weak correlation between training loss and real-world success rate.
		- Close loop evaluation procedure also pains research into agents. 
		- Slow eval significantly slows down the iteration.
		- What about eval in simulation?
			- sim2real gap: rigid/cloth/deformable
	- Challenges: data
		- What to collect, where to collect and how much to collect?
	- Challenges: FM for embodied agents
		- LLM/VLM are not tailored toward embodied agents
		- RL from embodied feedback
	- Adaptation and Life-long learning
	- Every robotics work is a system work, system 1 and system 2
	- ![](/Users/patrliu/Desktop/Screenshot\ 2025-10-20\ at\ 16.45.33.png)
	- Opinion from Yunzhu Li
		- High level language works, but gap still remain for low level contact/control.
		- Locomotion: you care about the ego status of the vehicle. Many consider locomotion a solved problem.
		- Manupulation: we care about the messy enviroment, and simulation of the environment is the key. 

## [1st Workshop on Multimodal Spatial Intelligence](https://musi-workshop.github.io/)
- Towards Spatial Supersensing, by Saining Xie
	- spatial reasning is falling short, 2/3 of the error. 1/3 is language understanding and vision error.
	- MLLM is still local models
	- ![](/Users/patrliu/Desktop/visual_bandwidth_saining_xie.jpg)

	
- Why is Spatial Reasoning Hard for VLM?
	- 	Model is memorizing the priors in the training data. 
	-  VLM is lacking geometry understanding
	-  We are linearize/serialize everything

