{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model Complexity\n",
    "Calculate the number of parameters in the linear model in the previous lesson (each input is a 28x28 pixel image and there are 10 classes: letters from A to J.)\n",
    "* Answer: 28x28x10 (W) + 1x10 = 7850\n",
    "\n",
    "Generally have (N+1)* K\n",
    "\n",
    "* Small number of parameters: generally (N+1)\\* K parameters, where N = number of inputs, K = number of outputs.\n",
    "* Interactions of inputs are limited because model is linear\n",
    "\n",
    "But they are \n",
    "* efficient (which makes them cheap and fast to run) and \n",
    "* stable. -> Can show mathematically that small changes in input can never yield big changes in output (|W| is bounded) Derivatives are also constant. \n",
    "\n",
    "Want to keep parameters in linear functions but want entire function to be non-linear. Cannot just multiply $W_1W_2W_3$ because that's equivalent to one linear function. So we have to introduce **non-linearities**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units (RELU)\n",
    "Non-linear function: Simplest non-linear function\n",
    "y = 0 when x <0. y = x when x>= 0.\n",
    "\n",
    "* How to use this (refer to our linear classifier process): Insert a RELU in the middle. Now have two matrices: One from the input to the RELU and one from the RELU to the output.\n",
    "* New parameter **H**: the number of RELUs you insert.\n",
    "\n",
    "Build network by stacking up simple operations to make the maths simple (**Chain Rule**).\n",
    "\n",
    "Can write the chain rule in a way that is computationally efficient.\n",
    "\n",
    "### How to compute derivatives: Back-Propagation \n",
    "\n",
    "Stochastic Gradient Descent: \n",
    "1. For each batch of data run forward prop and then back prop.\n",
    "2. That will give you gradients for each of the weights in your model.\n",
    "3. Apply gradients and learning rates to the original weights and update them.\n",
    "4. Repeat 1-3 many times to optimise your models.\n",
    "\n",
    "Note: Each block of the backprop typically takes twice the memory and computation of the forward prop blocks. -> Important for sizing your model and fitting it in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Deep Neural Network\n",
    "Increasing H is not efficient: you need to make it very big and then it gets hard to train.\n",
    "\n",
    "Instead, you can add more layers. A deep model is often preferred for two reasons:\n",
    "\n",
    "1. Parameter efficiency: Can generally get better performance with more parameters if you go **deeper** rather than **wider**.\n",
    "2. Many natural phenomena have a hierarchical structure. (E.g. Lines and edges -> Geometric shapes -> Objects in image recognition. Model matches abstractions you see in your data.)\n",
    "\n",
    "**Why did deep networks only become popular recently?** \n",
    "Deep models only really shine if you have large amounts of data to train them with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "Analogy: Skinny jeans are hard to get into, so people usually wear jeans that are a bit too big. Similarly, networks that's just the right size for your data are hard to train. (**Why?**) So in practice we train networks that are way too big for our data and then try our best to prevent them from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to prevent overfitting\n",
    "1. Early termination: Looking at performance on validation set and stop training once performance stops improving.\n",
    "2. Regularisation: Putting artificial constraints on your network that implicitly reduce the number of free parameters without making it more difficult to optimise. // Stretch pants.\n",
    "\n",
    "Two methods of regularisation:\n",
    "1. L2 Regularisation\n",
    "2. Dropout\n",
    "\n",
    "### L2 Regularisation\n",
    "Add term to the loss that penalises large weights, typically by adding L2 norm of your weights multiplied by a small constant to your loss. -> Additional hyperparameter to tune.\n",
    "* L2 norm: Sum of the squares of the elements in a vector.\n",
    "$$ L' = L + \\beta\\frac{1}{2}||W||^2_2 $$\n",
    "\n",
    "Pros:\n",
    "* Simple because you just add it to the loss. You don't need to change the structure of your network. \n",
    "\n",
    "### Dropout\n",
    "Recent new technique for regularisation. \n",
    "* Imagine if you have one layer connected to another layer. The values that go from one layer to the next are often called **activations**.\n",
    "* Take activations and for every example you train your network on, set half of them to zero. I.e. Randomly destroy half the data that's flowing through your network. Do this over and over. \n",
    "* Network cannot rely on any given activation to be present because it may get destroyed at any moment.\n",
    "* This forces network to learn **redundant representations** to ensure that at least some information remains. -> Seems inefficient but this makes things more **robust** and prevents overfitting. It also makes your network act as though it's taking a consensus over an ensemble of networks.\n",
    "\n",
    "Op: If dropout doesn't work for you, you should probably be using a bigger network.\n",
    "\n",
    "### Evaluating a Dropout-Trained Network\n",
    "When you evaluate the network that's been trained with dropout, you don't want randomness - you want something deterministic. You want the consensus. You get this by averaging activations. \n",
    "\n",
    "How do you get this? \n",
    "* During training, don't only zero. Scale remaining activations by factor of two. \n",
    "* When evaluating, remove dropout and scaling operations to get an average of the activations that is properly scaled.\n",
    "\n",
    "See more discussion [here](http://stats.stackexchange.com/questions/205932/dropout-scaling-the-activation-versus-inverting-the-dropout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
