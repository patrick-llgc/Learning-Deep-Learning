{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfer learning: repurposing a network for a different task.\n",
    "\n",
    "Bryan Catanzaro: wrote library than became CUDNN. Lol.\n",
    "\n",
    "GPU: Optimisod far throughput computation (simult)\n",
    "    - since update lots of pixels on screen at the same time.\n",
    "    - dl computations involve a lot of parallelisation.\n",
    "CPU: latency -> running a single thread of instructions as quickly as possible\n",
    "\n",
    "Rule of thumb: GPU 5x faster than CPU.\n",
    "\n",
    "\n",
    "Stanford University Notes on Transfer Learning: http://cs231n.github.io/transfer-learning/\n",
    "\n",
    "\n",
    "Why transfer learning is useful\n",
    "* Can use intel stored in trained networks to accelerate own progress\n",
    "- Your dataset may be small, so networks pre-trained on much data can help you generalise.\n",
    "\n",
    "DL more popular recently because of\n",
    "- Increased availability of labelled data\n",
    "- Increase in computational power\n",
    "\n",
    "- 1990s LeNet\n",
    "- ImageNet image classification competition \n",
    "    - 2012 AlexNet (Fundamental architecture\n",
    "        - Used parallelism of 40-byte GPUs to train network in about a week.\n",
    "        - Pioneered use of ReLUs in activation.\n",
    "        - Dropout to avoid overfitting.\n",
    "        - 15% error vs 26% error of winner in 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs\n",
    "Transfer Learning with TensorFlow\n",
    "Transfer learning is the practice of starting with a network that has already been trained, and then applying that network to your own problem.\n",
    "\n",
    "Because neural networks can often take days or even weeks to train, transfer learning (i.e. starting with a network that somebody else has already spent a lot of time training) can greatly shorten training time.\n",
    "\n",
    "So how we do go about applying transfer learning? Two popular methods are feature extraction and finetuning.\n",
    "\n",
    "- Feature extraction. We take a pretrained neural network and replace the final layer (classification layer) with a new classification layer for the new dataset or perhaps even a small neural network (eventually has a classification layer). During training the weights in all the old layers are frozen, only the weights for the new layer we added are trained. In other words, the gradient doesn't flow backwards past the first new layer we add.\n",
    "\n",
    "- Finetuning. Essentially the same as feature extraction except the weights of the old model aren't frozen. The network is trained end-to-end.\n",
    "In this lab and the one later in the section we're going to focus on feature extraction since it's less computationally intensive.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "The problem is that AlexNet was trained on the ImageNet database, which has 1000 classes of images. You can see the classes in the caffe_classes.py file. None of those classes involves traffic signs.\n",
    "\n",
    "In order to successfully classify our traffic sign images, you need to remove the final, 1000-neuron classification layer and replace it with a new, 43-neuron classification layer.\n",
    "\n",
    "This is called feature extraction, because you're basically extracting the images features captured by the penultimate layer, and passing them to a new classification layer.\n",
    "\n",
    "#### Solution\n",
    "First, I figure out the shape of the final fully connected layer, in my opinion this is the trickiest part. To do that I have to figure out the size of the output from fc7. Since it's a fully connected layer I know it's shape will be 2D so the second (or last) element of the list will be the size of the output. fc7.get_shape().as_list()[-1] does the trick. I then combine this with the number of classes for the Traffic Sign dataset to get the shape of the final fully connected layer, shape = (fc7.get_shape().as_list()[-1], nb_classes). The rest of the code is just the standard way to define a fully connected in TensorFlow. Finally, I calculate the probabilities via softmax, probs = tf.nn.softmax(logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from alexnet import AlexNet\n",
    "\n",
    "sign_names = pd.read_csv('signnames.csv')\n",
    "nb_classes = 43\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "# NOTE: By setting `feature_extract` to `True` we return\n",
    "# the second to last layer.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "\n",
    "# Define a new fully connected layer followed by a softmax activation to classify\n",
    "# the traffic signs. Assign the result of the softmax activation to `probs` below.\n",
    "\n",
    "# use this shape for the weight matrix\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)  \n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "\n",
    "# this is not a RELU layer since there is no nonlinear activation.\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference: of course the results would be unreliable\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (sign_names.ix[inds[-1 - i]][1], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Feature Extractor\n",
    "The feature extractor you just created works, in the sense that data will flow through the network and result in predictions.\n",
    "\n",
    "But the predictions aren't accurate, because you haven't yet trained the new classification layer.\n",
    "\n",
    "In order to do that, you'll need to read in the training dataset and train the network with cross entropy.\n",
    "\n",
    "Training AlexNet (even just the final layer!) can take a little while, so if you don't have a GPU, running on a subset of the data is a good alternative. As a point of reference one epoch over the training set takes roughly 53-55 seconds with a GTX 970.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified AlexNet used as a starting point for CV today\n",
    "\n",
    "Others:\n",
    "- Some feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet (VGG)\n",
    "\n",
    "From Visual Geometry group at Oxford.\n",
    "\n",
    "Architecture:\n",
    "* Long series of 3x3 convs \n",
    "* broken up by series of 2x2 pooling layers\n",
    "* finished by a trio of fully connected layers at the end.\n",
    "\n",
    "Starting point for working on other image classifcation tasks.\n",
    "VGG Strengths: \n",
    "* Flexibility\n",
    "\n",
    "Claim: Bias towards action needed to be a successful deep learning practitioner. Experiment continually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet\n",
    "\n",
    "* Runs really fast.\n",
    "* Inception module: trains well and is efficiently deployable.\n",
    "    - At each layer of your convnet, you can make a choice - pooling or convolution (1x1 or 3x3 or 5x5?) -> use all of them instead of choosing only one.\n",
    "    - Composition of averaging pooling followed by 1x1, then 1x1 followed by 3x3 ... and then concatenate the output.\n",
    "    - Choose params in such a way such that tho total number of parameters is small (so GoogLeNet runs nearly as fast as AlexNet,, can run in real time).\n",
    "    - -> Like ensembles.\n",
    "\n",
    "### ResNet (Microsoft)\n",
    "* 152 layers (vs AlexNet 8 layers, VGG 19, GoogLeNet 22)\n",
    "* Same structure repeated again and again like VGG. -> Add connections to NN that skill layers so\n",
    "* 3% on ImageNet, > human accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
